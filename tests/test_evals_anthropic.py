#!/usr/bin/env python3
"""Evaluation tests using Anthropic API to simulate real AI behavior."""

import os
import pytest
from typing import List, Dict, Optional, Any, cast
from dataclasses import dataclass
import anthropic  # type: ignore[import-not-found]

from mcp_nixos.server import (
    nixos_search,
    nixos_info,
    nixos_stats,
    home_manager_search,
    home_manager_info,
    home_manager_list_options,
    home_manager_options_by_prefix,
    darwin_search,
    darwin_info,
    darwin_list_options,
    darwin_options_by_prefix,
)


@dataclass
class EvalScenario:
    """Represents an evaluation scenario."""

    name: str
    user_query: str
    expected_behaviors: List[str]
    description: str = ""


@dataclass
class ToolCall:
    """Represents a tool call made by the AI."""

    name: str
    arguments: Dict[str, Any]
    result: str


@dataclass
class EvalResult:
    """Result of running an evaluation."""

    scenario: EvalScenario
    passed: bool
    score: float
    tool_calls: List[ToolCall]
    ai_response: str
    behaviors_observed: Dict[str, bool]
    reasoning: str


class AnthropicEvaluator:
    """Evaluates MCP tools using Anthropic API."""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        # Get API key from environment or parameter
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError(
                "No API key provided. Set ANTHROPIC_API_KEY environment variable or pass api_key parameter."
            )

        # Get model from environment or parameter, with default
        self.model = model or os.environ.get("ANTHROPIC_MODEL", "claude-sonnet-4-20250514")

        self.client = anthropic.Anthropic(api_key=self.api_key)

        # Available tools mapping
        self.tools = {
            "nixos_search": nixos_search,
            "nixos_info": nixos_info,
            "nixos_stats": nixos_stats,
            "home_manager_search": home_manager_search,
            "home_manager_info": home_manager_info,
            "home_manager_list_options": home_manager_list_options,
            "home_manager_options_by_prefix": home_manager_options_by_prefix,
            "darwin_search": darwin_search,
            "darwin_info": darwin_info,
            "darwin_list_options": darwin_list_options,
            "darwin_options_by_prefix": darwin_options_by_prefix,
        }

    def create_tools_description(self) -> List[Dict[str, Any]]:
        """Create tool descriptions for Claude."""
        return [
            {
                "name": "nixos_search",
                "description": (
                    "Search NixOS packages, options, or programs. "
                    "Use type='options' to find NixOS service configurations like services.nginx.*"
                ),
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "Search query"},
                        "type": {"type": "string", "enum": ["packages", "options", "programs"], "default": "packages"},
                        "limit": {"type": "integer", "default": 20, "minimum": 1, "maximum": 100},
                        "channel": {"type": "string", "default": "unstable"},
                    },
                    "required": ["query"],
                },
            },
            {
                "name": "nixos_info",
                "description": "Get detailed info about a NixOS package or option",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string", "description": "Package or option name"},
                        "type": {"type": "string", "enum": ["package", "option"], "default": "package"},
                        "channel": {"type": "string", "default": "unstable"},
                    },
                    "required": ["name"],
                },
            },
            {
                "name": "nixos_stats",
                "description": "Get NixOS statistics for a channel",
                "input_schema": {
                    "type": "object",
                    "properties": {"channel": {"type": "string", "default": "unstable"}},
                },
            },
            {
                "name": "home_manager_search",
                "description": "Search Home Manager configuration options for user-specific configurations",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "Search query"},
                        "limit": {"type": "integer", "default": 20, "minimum": 1, "maximum": 100},
                    },
                    "required": ["query"],
                },
            },
            {
                "name": "home_manager_info",
                "description": "Get details about a specific Home Manager option",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string", "description": "Option name (e.g., programs.git.enable)"}
                    },
                    "required": ["name"],
                },
            },
            {
                "name": "home_manager_list_options",
                "description": "List all Home Manager option categories",
                "input_schema": {"type": "object", "properties": {}},
            },
            {
                "name": "home_manager_options_by_prefix",
                "description": "Get all Home Manager options under a specific prefix",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "option_prefix": {"type": "string", "description": "Option prefix (e.g., programs.git)"}
                    },
                    "required": ["option_prefix"],
                },
            },
            {
                "name": "darwin_search",
                "description": "Search nix-darwin options for macOS-specific configurations",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "Search query"},
                        "limit": {"type": "integer", "default": 20, "minimum": 1, "maximum": 100},
                    },
                    "required": ["query"],
                },
            },
            {
                "name": "darwin_info",
                "description": "Get details about a specific nix-darwin option",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string", "description": "Option name (e.g., system.defaults.dock.autohide)"}
                    },
                    "required": ["name"],
                },
            },
            {
                "name": "darwin_list_options",
                "description": "List all nix-darwin option categories",
                "input_schema": {"type": "object", "properties": {}},
            },
            {
                "name": "darwin_options_by_prefix",
                "description": "Get all nix-darwin options under a specific prefix",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "option_prefix": {"type": "string", "description": "Option prefix (e.g., system.defaults.dock)"}
                    },
                    "required": ["option_prefix"],
                },
            },
        ]

    def run_scenario(self, scenario: EvalScenario) -> EvalResult:
        """Run a single evaluation scenario."""
        tool_calls: List[ToolCall] = []

        # Create system prompt
        system_prompt = (
            "You are a helpful assistant for NixOS users. You have access to tools to search "
            "and get information about NixOS packages, options, and configurations. Use these "
            "tools to provide accurate and helpful responses.\n\n"
            "When helping users:\n"
            "1. Search for relevant packages or options\n"
            "2. Get detailed information when needed\n"
            "3. Provide clear configuration examples\n"
            "4. Explain the differences between system-wide and user-specific configurations when relevant\n"
            "5. Include information about Home Manager or nix-darwin when appropriate for the user's needs"
        )

        # Create messages
        messages: List[Dict[str, Any]] = [{"role": "user", "content": scenario.user_query}]

        # Make API call with tools
        response = self.client.messages.create(
            model=self.model,
            max_tokens=1024,
            system=system_prompt,
            messages=cast(Any, messages),  # Cast to Any to satisfy type checker
            tools=cast(Any, self.create_tools_description()),  # Cast to Any to satisfy type checker
            tool_choice=cast(Any, {"type": "auto"}),  # Cast to Any to satisfy type checker
        )

        # Process initial response
        ai_response_parts = []
        tool_use_blocks = []

        for content in response.content:
            if content.type == "text":
                ai_response_parts.append(content.text)
            elif content.type == "tool_use":
                tool_use_blocks.append(content)

        # If there are tool calls, process them
        if tool_use_blocks:
            # Execute all tool calls
            tool_results = []
            for tool_use in tool_use_blocks:
                tool_name = tool_use.name
                tool_args = tool_use.input

                if tool_name in self.tools:
                    try:
                        result = self.tools[tool_name](**cast(Dict[str, Any], tool_args))
                    except Exception as e:
                        result = f"Error: {str(e)}"
                else:
                    result = f"Unknown tool: {tool_name}"

                tool_calls.append(ToolCall(name=tool_name, arguments=cast(Dict[str, Any], tool_args), result=result))

                tool_results.append({"type": "tool_result", "tool_use_id": tool_use.id, "content": result})

            # Continue conversation with all tool results
            messages.append({"role": "assistant", "content": response.content})
            messages.append({"role": "user", "content": tool_results})

            # Get final response after tool use - allow more tool calls if needed
            max_turns = 3  # Allow up to 3 turns of tool use
            turn = 0

            while turn < max_turns:
                final_response = self.client.messages.create(
                    model=self.model,
                    max_tokens=2048,
                    system=system_prompt,
                    messages=cast(Any, messages),  # Cast to Any to satisfy type checker
                    tools=cast(Any, self.create_tools_description()),  # Cast to Any to satisfy type checker
                    tool_choice=cast(
                        Any, {"type": "auto" if turn < max_turns - 1 else "none"}
                    ),  # Cast to Any to satisfy type checker
                )

                has_tool_use = False
                tool_results = []

                for final_content in final_response.content:
                    if final_content.type == "text":
                        ai_response_parts.append(final_content.text)
                    elif final_content.type == "tool_use" and turn < max_turns - 1:
                        has_tool_use = True
                        # Execute additional tool call
                        tool_name = final_content.name
                        tool_args = final_content.input

                        if tool_name in self.tools:
                            try:
                                result = self.tools[tool_name](**cast(Dict[str, Any], tool_args))
                            except Exception as e:
                                result = f"Error: {str(e)}"
                        else:
                            result = f"Unknown tool: {tool_name}"

                        tool_calls.append(
                            ToolCall(name=tool_name, arguments=cast(Dict[str, Any], tool_args), result=result)
                        )

                        tool_results.append({"type": "tool_result", "tool_use_id": final_content.id, "content": result})

                if has_tool_use:
                    # Add tool use and results to messages
                    messages.append({"role": "assistant", "content": final_response.content})
                    messages.append({"role": "user", "content": tool_results})
                    turn += 1
                else:
                    # No more tool use, we're done
                    break

        ai_response = "\n".join(ai_response_parts)

        # Evaluate behaviors
        behaviors_observed = self._evaluate_behaviors(scenario.expected_behaviors, tool_calls, ai_response)

        # Calculate score
        score = sum(behaviors_observed.values()) / len(behaviors_observed) if behaviors_observed else 0.0
        passed = score >= 0.8  # 80% threshold

        # Generate reasoning
        reasoning = self._generate_reasoning(behaviors_observed, tool_calls)

        return EvalResult(
            scenario=scenario,
            passed=passed,
            score=score,
            tool_calls=tool_calls,
            ai_response=ai_response,
            behaviors_observed=behaviors_observed,
            reasoning=reasoning,
        )

    def _evaluate_behaviors(
        self, expected_behaviors: List[str], tool_calls: List[ToolCall], ai_response: str
    ) -> Dict[str, bool]:
        """Evaluate which expected behaviors were observed."""
        behaviors = {}

        # Combine all tool call results and AI response for comprehensive checking
        all_content = ai_response.lower()
        for tc in tool_calls:
            all_content += "\n" + tc.result.lower()

        for behavior in expected_behaviors:
            if "tool call:" in behavior.lower():
                # Check for specific tool calls
                tool_name = behavior.split(":")[-1].strip()
                behaviors[behavior] = any(tc.name == tool_name for tc in tool_calls)
            elif "mentions" in behavior.lower():
                # Check for mentions in response or tool results
                keyword = behavior.split("mentions")[-1].strip()
                # Handle multi-word phrases
                behaviors[behavior] = keyword.lower() in all_content
            elif "provides" in behavior.lower() and "example" in behavior.lower():
                # Check if code examples are provided
                behaviors[behavior] = any(
                    marker in ai_response for marker in ["```", "pkgs.", "services.", "programs.", "{ ", "};"]
                )
            elif "provides" in behavior.lower() and "recommendation" in behavior.lower():
                # Check if recommendation is provided
                behaviors[behavior] = any(
                    word in ai_response.lower()
                    for word in ["recommend", "better", "suggest", "should", "prefer", "advise"]
                )
            elif "explains" in behavior.lower():
                # Check for explanations (heuristic)
                topic = behavior.split("explains")[-1].strip()
                # Look for explanatory patterns
                explanatory_words = [
                    "because",
                    "since",
                    "allows",
                    "means",
                    "provides",
                    "enables",
                    "differs",
                    "whereas",
                    "while",
                ]
                has_explanation = any(word in ai_response.lower() for word in explanatory_words)
                behaviors[behavior] = topic.lower() in all_content and has_explanation and len(ai_response) > 100
            elif "adding user to" in behavior.lower():
                # Check for specific group membership mentions
                group = behavior.split("to")[-1].strip()
                behaviors[behavior] = group.lower() in all_content and (
                    "users.users" in all_content or "extraGroups" in all_content
                )
            else:
                # Generic check in all content
                behaviors[behavior] = behavior.lower() in all_content

        return behaviors

    def _generate_reasoning(self, behaviors: Dict[str, bool], tool_calls: List[ToolCall]) -> str:
        """Generate reasoning about the evaluation result."""
        total = len(behaviors)
        passed = sum(behaviors.values())

        reasoning = f"Observed {passed}/{total} expected behaviors. "
        reasoning += f"Made {len(tool_calls)} tool calls. "

        failed_behaviors = [b for b, v in behaviors.items() if not v]
        if failed_behaviors:
            reasoning += f"Missing: {', '.join(failed_behaviors[:3])}"
            if len(failed_behaviors) > 3:
                reasoning += f" and {len(failed_behaviors) - 3} more"

        return reasoning


# Test scenarios
EVAL_SCENARIOS = [
    EvalScenario(
        name="install_vscode",
        user_query="I want to install VSCode on NixOS",
        expected_behaviors=[
            "tool call: nixos_search",
            "tool call: nixos_info",
            "mentions configuration.nix",
            "mentions environment.systemPackages",
            "provides installation example",
        ],
        description="Test package installation guidance",
    ),
    EvalScenario(
        name="nginx_setup",
        user_query="How do I set up nginx on NixOS to serve static files?",
        expected_behaviors=[
            "tool call: nixos_search",
            "mentions services.nginx.enable",
            "mentions virtualHosts",
            "provides configuration example",
            "mentions firewall",
        ],
        description="Test service configuration guidance",
    ),
    EvalScenario(
        name="git_home_manager",
        user_query="Should I configure git using NixOS or Home Manager?",
        expected_behaviors=[
            "tool call: nixos_search",
            "tool call: home_manager_search",
            "explains system vs user configuration",
            "mentions programs.git",
            "provides recommendation",
        ],
        description="Test Home Manager integration understanding",
    ),
    EvalScenario(
        name="docker_service",
        user_query="How do I enable Docker on NixOS?",
        expected_behaviors=[
            "tool call: nixos_search",
            "mentions virtualisation.docker.enable",
            "mentions adding user to docker group",
            "provides configuration example",
        ],
        description="Test Docker service configuration",
    ),
    EvalScenario(
        name="darwin_dock",
        user_query="How can I configure macOS dock settings with nix-darwin?",
        expected_behaviors=[
            "tool call: darwin_search",
            "mentions system.defaults.dock",
            "provides darwin-configuration.nix example",
            "mentions darwin-rebuild",
        ],
        description="Test macOS nix-darwin configuration",
    ),
    EvalScenario(
        name="flake_development",
        user_query="What's the best way to set up a development environment for a Python project on NixOS?",
        expected_behaviors=[
            "tool call: nixos_search",
            "mentions development shells",
            "mentions flakes or shell.nix",
            "provides example configuration",
        ],
        description="Test development environment guidance",
    ),
]


# Load .env file for tests
from dotenv import load_dotenv  # type: ignore[import-not-found]

load_dotenv()


@pytest.mark.anthropic
@pytest.mark.skipif(
    not os.environ.get("ANTHROPIC_API_KEY"), reason="ANTHROPIC_API_KEY not set - see README.md for setup instructions"
)
class TestAnthropicEvals:
    """Run evaluations using Anthropic API."""

    @pytest.fixture
    def evaluator(self):
        """Create evaluator instance."""
        return AnthropicEvaluator()

    def test_package_installation(self, evaluator):
        """Test package installation scenario."""
        scenario = EVAL_SCENARIOS[0]
        result = evaluator.run_scenario(scenario)

        # Print results for debugging
        print(f"\nScenario: {scenario.name}")
        print(f"Score: {result.score:.2f}")
        print(f"Tool calls: {len(result.tool_calls)}")
        print(f"Reasoning: {result.reasoning}")
        print(f"Behaviors observed: {result.behaviors_observed}")

        assert result.score >= 0.5  # Lower threshold for real AI variability
        assert len(result.tool_calls) > 0
        assert any("vscode" in str(tc.arguments).lower() for tc in result.tool_calls)

    def test_service_configuration(self, evaluator):
        """Test service configuration scenario."""
        scenario = EVAL_SCENARIOS[1]
        result = evaluator.run_scenario(scenario)

        print(f"\nScenario: {scenario.name}")
        print(f"Score: {result.score:.2f}")
        print(f"Tool calls: {len(result.tool_calls)}")
        print(f"Reasoning: {result.reasoning}")
        print(f"Behaviors observed: {result.behaviors_observed}")

        assert result.score >= 0.5
        assert any(tc.name == "nixos_search" and tc.arguments.get("type") == "options" for tc in result.tool_calls)

    def test_home_manager_integration(self, evaluator):
        """Test Home Manager integration scenario."""
        scenario = EVAL_SCENARIOS[2]
        result = evaluator.run_scenario(scenario)

        print(f"\nScenario: {scenario.name}")
        print(f"Score: {result.score:.2f}")
        print(f"Tool calls: {len(result.tool_calls)}")
        print(f"Reasoning: {result.reasoning}")
        print(f"Behaviors observed: {result.behaviors_observed}")

        assert result.score >= 0.5
        assert any("home_manager" in tc.name for tc in result.tool_calls)

    def test_docker_service(self, evaluator):
        """Test Docker service configuration."""
        scenario = EVAL_SCENARIOS[3]
        result = evaluator.run_scenario(scenario)

        print(f"\nScenario: {scenario.name}")
        print(f"Score: {result.score:.2f}")
        print(f"Tool calls: {len(result.tool_calls)}")
        print(f"Reasoning: {result.reasoning}")

        assert result.score >= 0.5
        assert any("docker" in str(tc.arguments).lower() for tc in result.tool_calls)

    def test_darwin_configuration(self, evaluator):
        """Test macOS nix-darwin configuration."""
        scenario = EVAL_SCENARIOS[4]
        result = evaluator.run_scenario(scenario)

        print(f"\nScenario: {scenario.name}")
        print(f"Score: {result.score:.2f}")
        print(f"Tool calls: {len(result.tool_calls)}")
        print(f"Reasoning: {result.reasoning}")

        assert result.score >= 0.5
        assert any("darwin" in tc.name for tc in result.tool_calls)

    def test_development_environment(self, evaluator):
        """Test development environment guidance."""
        scenario = EVAL_SCENARIOS[5]
        result = evaluator.run_scenario(scenario)

        print(f"\nScenario: {scenario.name}")
        print(f"Score: {result.score:.2f}")
        print(f"Tool calls: {len(result.tool_calls)}")
        print(f"Reasoning: {result.reasoning}")

        assert result.score >= 0.25  # Very low threshold for complex scenario
        assert len(result.tool_calls) > 0


def generate_evaluation_report(results: List[EvalResult]) -> str:
    """Generate a comprehensive evaluation report."""
    total = len(results)
    passed = sum(1 for r in results if r.passed)
    avg_score = sum(r.score for r in results) / total if total > 0 else 0

    report = f"""# MCP-NixOS Evaluation Report

## Summary
- Total Scenarios: {total}
- Passed: {passed} ({passed/total*100:.1f}%)
- Average Score: {avg_score:.2%}
- Minimum Pass Threshold: 50%

## Detailed Results
"""

    for result in results:
        status = "✅ PASS" if result.passed else "❌ FAIL"
        report += f"\n### {status} {result.scenario.name} (Score: {result.score:.2%})\n"
        report += f"**Description**: {result.scenario.description}\n"
        report += f"**Query**: {result.scenario.user_query}\n"
        report += f"**Tool Calls**: {len(result.tool_calls)}\n"

        # Show which behaviors were observed
        report += "\n**Behaviors**:\n"
        for behavior, observed in result.behaviors_observed.items():
            check = "✓" if observed else "✗"
            report += f"- {check} {behavior}\n"

        report += f"\n**Reasoning**: {result.reasoning}\n"
        report += "-" * 60 + "\n"

    return report


if __name__ == "__main__":
    # Run all scenarios and generate report
    if api_key := os.environ.get("ANTHROPIC_API_KEY"):
        evaluator = AnthropicEvaluator(api_key)
        results = []

        print("Running MCP-NixOS evaluation scenarios...")
        print("=" * 60)

        for i, scenario in enumerate(EVAL_SCENARIOS):
            print(f"\n[{i+1}/{len(EVAL_SCENARIOS)}] Running: {scenario.name}")
            print(f"Query: {scenario.user_query}")

            try:
                result = evaluator.run_scenario(scenario)
                results.append(result)

                status = "PASS" if result.passed else "FAIL"
                print(f"Result: {status} (Score: {result.score:.2%})")
                print(f"Tool calls made: {len(result.tool_calls)}")

                # Show missed behaviors
                missed = [b for b, o in result.behaviors_observed.items() if not o]
                if missed:
                    print(f"Missed behaviors: {', '.join(missed[:3])}")
                    if len(missed) > 3:
                        print(f"  ... and {len(missed) - 3} more")

            except Exception as e:
                print(f"ERROR: {str(e)}")
                # Create a failed result
                result = EvalResult(
                    scenario=scenario,
                    passed=False,
                    score=0.0,
                    tool_calls=[],
                    ai_response=f"Error: {str(e)}",
                    behaviors_observed={b: False for b in scenario.expected_behaviors},
                    reasoning=f"Test failed with error: {str(e)}",
                )
                results.append(result)

        print("\n" + "=" * 60)

        # Generate and print report
        report = generate_evaluation_report(results)
        print("\n" + report)

        # Save report to file
        with open("evaluation_report.md", "w") as f:
            f.write(report)
        print("\nReport saved to evaluation_report.md")

        # Overall pass/fail
        overall_passed = sum(1 for r in results if r.passed) >= len(results) * 0.7
        if overall_passed:
            print("\n✅ Overall evaluation PASSED")
        else:
            print("\n❌ Overall evaluation FAILED")

    else:
        print("Set ANTHROPIC_API_KEY environment variable to run tests")
